---
title: 'A Bit on the Nose (Draft)'
author: Greg Foletta
date: '2021-02-04'
slug: a-bit-on-the-nose
categories: [R]
---

I've never been particularly interested in horse racing, however I've married into a family that's mad for it. Each of them has their own ideas and combinations of factors that lead them to bet on a particular horse; it could be form, barrier position, track condition, and many others.

I was interested at coming at the problem from a data driven perspective. I must admit that there was an initial arrogance that overcame me; an assumption that "of course I can do this better". In fact I've seen this in many places where 'data scientists' stroll into areas with the tools and data, but a lack of understanding of the problem space. Poor assumptions surely abound, and incorrect conclusions are almost certainly reached.

After I quashed this misplaced sense of superiority, I started to think about how I could compute naive baselines that any models I may create could be compared against. The first naive model is to pick a random horse in each race. This is the lower bound for model predictive accuracy. The second naive model is to pick the favourite in each race. The favourite has many of the factors that we would be using in the model already built in: form, barrier position, trainer, jockey, etc. Any model we create needs to do better than this, otherwise we're no better than consensus.

In this article I will answer the following question:

> What is the mean and distribution of returns, and the predictive accuracy, of betting on a random and the favourite respectively in a horse race?


```{r include=FALSE}
library(tidyverse)
library(rsample)
library(encryptr)
library(gt)
library(zip)
library(glue)
library(furrr)
library(lubridate)
```


# Data Information & Aquisition

```{r include=FALSE}
hr_col_types <- cols(
    race_id = col_double(),
    track = col_character(),
    state = col_character(),
    results_link = col_character(),
    date = col_date(format = ""),
    raceday_link = col_character(),
    race_number = col_double(),
    position = col_character(),
    horse.name = col_character(),
    horse.number = col_double(),
    barrier = col_double(),
    margin = col_double(),
    weight = col_double(),
    horse.age = col_double(),
    horse.type = col_character(),
    trainer = col_character(),
    jockey = col_character(),
    horse.ancestry = col_character(),
    odds.sp = col_double(),
    odds.stab = col_double(),
    odds.nsw = col_double(),
    odds.ubet = col_double(),
    odds.sb = col_double(),
    race_duration = col_character(),
    race_datetime = col_datetime(),
    rail_position = col_character(),
    race_name = col_character(),
    length = col_double(),
    class = col_character(),
    condition = col_character(),
    error = col_character(),
    track_race_id = col_double(),
    result = col_character(),
    condition.num = col_double(),
    odds.sp.win = col_double()
)

results_file <- 'hr_results.csv.zip.encryptr.bin'

if(!file.exists(results_file)) {
    download.file(
        url = glue('https://gregfoletta.s3-ap-southeast-2.amazonaws.com/articles-data/{results_file}'),
        destfile = results_file
    )
}

decrypt_file(results_file)
unzip(str_remove(results_file, '.encryptr.bin'))

hr_results <- read_csv(
    'hr_results.csv',
    col_types = hr_col_types
)

file.remove('hr_results.csv')
file.remove('hr_results.csv.zip')
```

The data was acquired by using [rvest](https://rvest.tidyverse.org/) to scrape a website that contained the information I needed. I was able to iterate across each each , pulling out specific variables using CSS selectors and XPaths. The dataset is for my own personal use, and I have encrypted the data that us used in this article.

The dataset contains information on around 180,000 horse races over the period of 2011 to 2020. The data is in a tidy format, with each row containing information on each horse in each race. This information includes the name and state that the track, the date of the race, the name of the horse, jockey and trainer, the weight the horse is carrying, race length, duration, barrier position, etc.

Most of this information won't be used, instead we'll be focusing on the following key variables:

- *race_id* - a unique identifier for each race. There are multiple rows with the same *race_id*, each representing a horse that ran in that race.
- *odds.sp* - the 'starting price', which is are the "odds prevailing on a particular horse in the on-course fixed-odds betting market at the time a race begins.".
- *position* - the finishing position of the horse.

For readability I won't show all of the code used, however as always the full source of this page (and the entire website) is available on [github](https://github.com/gregfoletta/articles.foletta.org). The dataset is downloaded, decrypted, unzipped, the loaded into the variable `hr_results`:

```{r}
hr_results %>% 
    select(race_id, state, track, horse.name, jockey, odds.sp, position, barrier, weight) %>% 
    slice_sample(n = 10) %>% 
    gt()
```


# An Explore

Let's take a look at the dataset from a few different perspectives to give us some context. First up we take a look at the number of races per month per state. We can clearly see the yearly cyclic nature, with the rise into the spring racing carnivals and a drop off over winter.

```{r}
hr_results %>% 
    count(state, month = floor_date(date, '1 month')) %>% 
    ggplot() +
    geom_line(aes(month, n, colour = state)) +
    labs(
        title = 'Number of Race Days per Month per State',
        x = 'Month',
        y = 'Race Days'
    )
```

Next we take a look at the top 10 winning horses and trainers over this period:

```{r}
hr_results %>%
    filter(position == 1) %>%  
    pivot_longer(c(horse.name, trainer)) %>% 
    count(name, value, name = 'wins') %>% 
    group_by(name) %>%
    slice_max(wins, n = 10) %>% 
    ggplot() +
    geom_col(aes(fct_reorder(value, wins), wins), fill = 'darkgreen') +
    facet_wrap(vars(name), scales = 'free') +
    coord_flip() +
    labs(
        title = 'Top 10 Wins by Horse and by Trainer (2011 - 2020)',
        x = 'Name',
        y = 'Wins'
    )
```

Finally, which tracks have run the most races over this period?

```{r}
hr_results %>% 
    distinct(race_id, .keep_all = TRUE) %>% 
    count(track, name = 'races') %>% 
    slice_max(races, n = 10) %>% 
    ggplot() +
    geom_col(aes(fct_reorder(track, races), races), fill = 'lightblue') +
    coord_flip() +
    labs(
        title = 'Tracks - Total Race Days (2011 - 2020)',
        x = 'Tack Name',
        y = 'Race Days'
    )
```

# Data Sampling

With a high level handle on the data we're working with, let's move on to answering the questions. The process is reasonably simple: we simulate placing a dollar bet on a horse, and either collect our return (the starting price odds) in the event that in won, or lose the dollar in the event that it lost.

We'll do this in a manner that tries to be realistic, so we're not going to bet on every race. We also want to determine the variability, so there will need to be a random element in there are well. The process is:
- A sample of races across the time period is taken.
    - I'll be using 0.5% of all races, which works out to be ~800 races, or about 1.5 races bet on per week.
- A dollar 'bet' is placed (based on the two criteria) on a horse in each race.
- The return is determined.
- The cumulative return is calculated.
- This process is repeated a number of times. 

This in effect allows us to live a number of 'lives' over the period, with each life varying in the races that were bet on.

The first step is to nest the data for each race together, allowing us to easily sample on a per race basis, rather than on a per horse basis.

```{r}
# Nest per race
hr_results <- hr_results %>%
    group_by(race_id) %>% 
    nest()

head(hr_results)
```

The `mc_cv()` (Monte-Carlo cross validation) function is used to create our sampled data sets. Technically I'm not performing the cross-validation part, as I'm only using the 'analysis' or 'training' set of sampled data. I'm using it as a way to take repeated samples from the data set.

We create a worker function so we can pass it to `future_map()`, which allows us to spread the workload across multiple cores on the system, speeding up the process.

The returned results are then unnested, returning us back to our original tidy format, with each sample identified by the *sample_id* variable.

```{r}
# Sampling function 
mc_sample <- function(data, times, prop) {
    data %>% 
        mc_cv(times = times, prop = prop) %>% 
        mutate(analysis = map(splits, ~analysis(.x))) %>%
        select(-c(id, splits))
}

plan(multisession, workers = availableCores() - 1)

# Parallel Monte Carlo cross-validation
number_samples <- 800
hr_mccv <- future_map(
    1:number_samples,
    ~{ mc_sample(hr_results, times = 1, prop = .005) },
    .options = furrr_options(seed = TRUE)
)

# Switch plans to close workers and release memory
plan(sequential)

# Bind samples together and unnest
hr_mccv <- hr_mccv %>% 
    bind_rows() %>% 
    mutate(sample_id = 1:n()) %>% 
    unnest(cols = analysis) %>% 
    unnest(cols = data)
```

A `dollar_bets()` function is created which takes our horses (one per race) that we've 'bet' on, determines the return for each horse, and then on a per sample basis indexes the races and calculates the cumulative return. This index is used as our x-axis in some of the graphs, as we can't use date due to the sampled nature of the data.

```{r}
dollar_bets <- function(data) {
    data %>% 
        mutate(bet.return = if_else(position == 1, odds.sp, -1)) %>% 
        group_by(sample_id) %>% 
        mutate(
            sample_race_index = 1:n(),
            cumulative.return = cumsum(bet.return)
        ) %>% 
        ungroup()
}
```

# Approach 1: Random Selection

The first approach to take is to bet on a random horse per race.

```{r}
# Random horse from each race
hr_random <- hr_mccv %>% 
    drop_na(odds.sp) %>%
    group_by(sample_id, race_id) %>% 
    slice_sample(n = 1) %>% 
    ungroup()

# Place our bets
hr_random <- dollar_bets(hr_random)
```

Looking at the cumulative return over time and the distribution of returns.

```{r}
hr_random %>% 
    filter(sample_id %in% 1:40) %>% 
    ggplot() +
    geom_line(aes(sample_race_index, cumulative.return, group = sample_id), alpha = .5) +
    labs(
        title = "Dollar Bets - Random",
        x = 'Race Index',
        y = 'Dollars'
    )
hr_random %>% 
    group_by(sample_id) %>% 
    summarise(return = sum(bet.return)) %>%
    ggplot() +
    geom_histogram(aes(return), binwidth = 5) +
    geom_vline(aes(xintercept = mean(return))) +
    labs(
        title = 'Dollar Bet - Random Horse - Returns Over Time',
        x = 'Sample Race Index',
        y = 'Dollars'
    )
```
```{r include = FALSE}
sample_returns <- hr_random %>%
    group_by(sample_id) %>% 
    summarise(return = sum(bet.return)) %>%
    pull(return)

mean_return <- mean(sample_returns)
ninefive_percent_range <- sample_returns %>%
    quantile(probs = c(.025, .975)) %>% 
    glue_collapse(sep = '-')
```


The mean return on this bet is `r mean_return` with the 95% of returns in the range of `r ninefive_percent_range` 

# Approach 2 - Favourite

The second approach to take is to bet on the favourite in each race.

```{r}
# Favourite horse from each race
hr_favourite <- hr_mccv %>% 
    drop_na(odds.sp) %>% 
    group_by(sample_id, race_id) %>% 
    mutate(odds.rank = order(odds.sp)) %>% 
    slice_min(odds.rank, with_ties = FALSE, n = 1) %>% 
    ungroup()
    
hr_favourite <- hr_favourite %>% 
    mutate(bet.return = if_else(position == 1, odds.sp, -1)) %>% 
    group_by(sample_id) %>% 
    mutate(
        sample_race_index = 1:n(),
        cumulative.return = cumsum(bet.return)
    ) %>% 
    ungroup()
```

Again we look at the cumulative return over time, and the distribution of returns.

```{r}
hr_favourite %>%
    filter(sample_id %in% 1:40) %>% 
    ggplot() +
    geom_line(aes(sample_race_index, cumulative.return, group = sample_id), alpha = .5) +
    labs(
        title = "Dollar Bets - Favourites",
        x = 'Race'
    )

hr_favourite %>% 
    group_by(sample_id) %>% 
    summarise(return = sum(bet.return)) %>%
    ggplot() +
    geom_histogram(aes(return), binwidth = 5) +
    geom_vline(aes(xintercept = mean(return))) +
    labs(
        title = 'Dollar Bet - Favourite - Returns',
        x = 'Sample Race Index',
        y = 'Dollars'
    )
```

```{r include = FALSE}
sample_returns <- hr_favourite %>%
    group_by(sample_id) %>% 
    summarise(return = sum(bet.return)) %>%
    pull(return)

mean_return <- mean(sample_returns)
ninefive_percent_range <- sample_returns %>%
    quantile(probs = c(.025, .975)) %>% 
    glue_collapse(sep = '-')
```


The mean return on this bet is `r mean_return` with the 95% of returns in the range of `r ninefive_percent_range` 

# Summary

Favourite looks good.