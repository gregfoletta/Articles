---
title: 'A Bit on the Nose (Draft)'
author: Greg Foletta
date: '2021-0215'
slug: a-bit-on-the-nose
categories: [R]
---

I've never been particularly interested in horse racing, but I married into a family that loves it. Each in-law has their own ideas and combinations of factors that lead them to bet on a particular horse; be it form, barrier position, track condition, trainer, jockey, or or more.

After being drawn into conversations about their preferred selection methods, I wanted come at the problem backed with data. I must admit I had an initial feeling of arrogance, thinking "of course I can do this better". In fact I've seen this in many places where 'data scientists' stroll into fields of enquiry armed with data and a swag of models, but lacking an understanding of the problem space. Poor assumptions abound, and incorrect conclusions are almost certainly reached.

I was determined not to fall into the same traps, and after quashing my misplaced sense of superiority, I started to think about how to approach the problem at hand. Rather than diving straight into prediction - models akimbo - I thought the best place to start would be to create naive baseline models of the data. This would give me something to compare the performance of any subsequent models created.

In this article I will look at two naive 'models'. The first is to simply to pick a random horse in each race. This is the lower bound for model predictive accuracy. The second is to pick the favourite in each race. The favourite has many of the factors that we would be using in the model already built in via the consensus of all of the bettors: form, barrier position, trainer, jockey, etc. Any model we create needs to approach the accuracy of this method.

Simply put, we want to answer the following questions for both of these two methods:

> What is the mean and middle ninety-five percentile accuracy?

> What is the mean and middle ninety-five percentile profit per race?


```{r include=FALSE}
library(tidyverse)
library(rsample)
library(encryptr)
library(gt)
library(zip)
library(glue)
library(furrr)
library(lubridate)
library(scales)
```


# Data Information & Aquisition

```{r include=FALSE}
hr_col_types <- cols(
    race_id = col_double(),
    track = col_character(),
    state = col_character(),
    results_link = col_character(),
    date = col_date(format = ""),
    raceday_link = col_character(),
    race_number = col_double(),
    position = col_character(),
    horse.name = col_character(),
    horse.number = col_double(),
    barrier = col_double(),
    margin = col_double(),
    weight = col_double(),
    horse.age = col_double(),
    horse.type = col_character(),
    trainer = col_character(),
    jockey = col_character(),
    horse.ancestry = col_character(),
    odds.sp = col_double(),
    odds.stab = col_double(),
    odds.nsw = col_double(),
    odds.ubet = col_double(),
    odds.sb = col_double(),
    race_duration = col_character(),
    race_datetime = col_datetime(),
    rail_position = col_character(),
    race_name = col_character(),
    length = col_double(),
    class = col_character(),
    condition = col_character(),
    error = col_character(),
    track_race_id = col_double(),
    result = col_character(),
    condition.num = col_double(),
    odds.sp.win = col_double()
)

results_file <- 'hr_results.csv.zip.encryptr.bin'

if(!file.exists(results_file)) {
    download.file(
        url = glue('https://gregfoletta.s3-ap-southeast-2.amazonaws.com/articles-data/{results_file}'),
        destfile = results_file
    )
}

decrypt_file(results_file)
unzip(str_remove(results_file, '.encryptr.bin'))

hr_results <- read_csv(
    'hr_results.csv',
    col_types = hr_col_types
)

file.remove('hr_results.csv')
file.remove('hr_results.csv.zip')
```

The data was acquired by using [rvest](https://rvest.tidyverse.org/) to scrape a website that contained historical information on horse races. I was able to iterate across each race, pulling out specific variables using CSS selectors and XPaths. The dataset is for my own personal use, and I have encrypted the data that us used in this article.

The dataset contains information on around 180,000 horse races over the period from 2011 to 2020. The data is in a tidy format, with each row containing information on each horse in each race. This information includes the name and state that the track, the date of the race, the name of the horse, jockey and trainer, the weight the horse is carrying, race length, duration, barrier position. Here's an random sample from the dataset:

```{r}
hr_results %>% 
    select(race_id, state, track, horse.name, jockey, odds.sp, position, barrier, weight) %>% 
    slice_sample(n = 10) %>% 
    gt()
```

We won't use most of the variables in the data set, only a select few:

- *race_id* - a unique identifier for each race. There are multiple rows with the same *race_id*, each representing a horse that ran in that race.
- *odds.sp* - the 'starting price', which is are the "odds prevailing on a particular horse in the on-course fixed-odds betting market at the time a race begins.".
- *position* - the finishing position of the horse.

The code to load the data is not shown, however the full source of this article (and the entire website) is available at [github](https://github.com/gregfoletta/articles.foletta.org). The data is contained in the variable `hr_results`.


# Exploration

Let's take a look at the dataset from a few different perspectives to give us some context. First up we take a look at the number of races per month per state. We can clearly see the yearly cyclic nature, with the rise into the spring racing carnivals and a drop off over winter.

```{r}
hr_results %>% 
    count(state, month = floor_date(date, '1 month')) %>% 
    ggplot() +
    geom_line(aes(month, n, colour = fct_reorder(state, n, .desc = TRUE))) +
    labs(
        title = 'Number of Race Days per Month per State',
        x = 'Month',
        y = 'Race Days',
        colour = 'State'
    )
```

Next we take a look at the top 10 winning horses and trainers over this period:

```{r}
hr_results %>%
    filter(position == 1) %>%  
    pivot_longer(c(horse.name, trainer)) %>% 
    count(name, value, name = 'wins') %>% 
    group_by(name) %>%
    slice_max(wins, n = 10) %>% 
    ggplot() +
    geom_col(aes(fct_reorder(value, wins), wins), fill = 'darkgreen') +
    facet_wrap(vars(name), scales = 'free') +
    coord_flip() +
    labs(
        title = 'Top 10 Wins by Horse and by Trainer (2011 - 2020)',
        x = 'Name',
        y = 'Wins'
    )
```

Which tracks have run the most races over this period?

```{r}
hr_results %>% 
    distinct(race_id, .keep_all = TRUE) %>% 
    count(track, name = 'races') %>% 
    slice_max(races, n = 10) %>% 
    ggplot() +
    geom_col(aes(fct_reorder(track, races), races), fill = 'lightblue') +
    coord_flip() +
    labs(
        title = 'Tracks - Total Race Days (2011 - 2020)',
        x = 'Tack Name',
        y = 'Race Days'
    )
```

Finally, what is the distribution of the starting price odds? THere's a very long tail for this variable going up to around 500:1, so I've removed odds below 100:1 to provide a better view of the most common values. What's interesting to note there is the large increases in horses with odds on nice round numbers after the 20:1 mark.

```{r, message=FALSE}
hr_results %>%
    drop_na(odds.sp) %>%
    ggplot() +
    geom_histogram(aes(odds.sp), binwidth = 1) +
    scale_x_continuous(limits = c(0, 100), breaks = seq(0, 100, 5)) +
    labs(
        title = 'Starting Price Odds (2011 - 2021)',
        subtitle = 'Histogram',
        x = 'Odds',
        y = 'Count'
    )
```

# Data Sampling

With a high level handle on the data we're working with, let's move on to answering the questions.  The process used is as follows:

- A sample of races across the time period is taken.
    - We will use 0.5% of all races, which works out to be ~800 races.
- A dollar 'bet' is placed (based on the two criteria) on a horse in each race.
- The profit is determined, i.e. return - stake.
- The cumulative return per race is calculated. 
- The accuracy across the races is calculated.

This process is repeated a number of times across different samples, a la bootstrapping but without replacement. This allows us to determine not just the mean accuracy and profit per race, but also their distributions.

We first nest the data for each race together, allowing us to sample on a per race basis, rather than on a per horse basis:

```{r}
# Nest per race
hr_results <- hr_results %>%
    group_by(race_id) %>% 
    nest()

head(hr_results)
```


The the `mc_cv()` (Monte-Carlo cross validation) function to create our sampled data sets. Technically we're not performing the cross-validation part, only using the training set that comes back from the function and throwing away the test set.

The worker function `mc_sample()` is created to be passed to `future_map()`. The sampling is an 'embarrassingly parallel' task, so we would remiss to not use all the compute available to us.

We generate 20 samples (to be increased in the final version of this artucle) of .5% of the total races (~800). The returned results are unnested, returning us back to our original tidy format, with each sample identified by the *sample_id* variable:

```{r}
# Sampling function 
mc_sample <- function(data, times, prop) {
    data %>% 
        mc_cv(times = times, prop = prop) %>% 
        mutate(analysis = map(splits, ~analysis(.x))) %>%
        select(-c(id, splits))
}

# Set up out workers
plan(multisession, workers = availableCores() - 1)

# Parallel sampling
number_samples <- 800
hr_mccv <- future_map(
    1:number_samples,
    ~{ mc_sample(hr_results, times = 1, prop = .005) },
    .options = furrr_options(seed = TRUE)
)

# Switch plans to close workers and release memory
plan(sequential)

# Bind samples together and unnest
hr_mccv <- hr_mccv %>% 
    bind_rows() %>% 
    mutate(sample_id = 1:n()) %>% 
    unnest(cols = analysis) %>% 
    unnest(cols = data)
```


The `bet_ppr()` function places - by default a \$1 bet -  'on the nose' (i.e. for the win only) on each horse. It then determines the profit of our bet based on the starting price odds. The data set uses decimal (also known as continental) odds, so if we placed a \$1 bet on a horse with odds of 3.0 and the horse wins, our *return* is \$3, but our *profit* is \$2 (stake - \$1 be). If the horse doesn't win, our return is \$0 and our profit is -$1. 

```{r}
# Places a bet for the win on each horse and calculates the profit.
# For each sample of races it creates an index variable, and calculates
# the cumulative profit per race (ppr)
bet_ppr <- function(data, bet = 1) {
    data %>% 
        mutate(bet.profit = if_else(
                position == 1,
                (bet * odds.sp) - bet,
                -bet
            )
        ) %>% 
        group_by(sample_id) %>% 
        mutate(
            sample_race_index = 1:n(),
            cumulative.ppr = cumsum(bet.profit) / sample_race_index 
        ) %>% 
        ungroup()
}
```

# Approach 1: Random Selection

The first approach to take is to bet on a random horse per race.

```{r}
# Select a random horse from each race where there are odds available
hr_random <- hr_mccv %>% 
    drop_na(odds.sp) %>%
    group_by(sample_id, race_id) %>% 
    slice_sample(n = 1) %>% 
    ungroup()

# Place our bets
hr_random <- bet_ppr(hr_random)
```

What kind of accuracy does this give us?

```{r, message=FALSE}
hr_random_accuracy <- 
    hr_random %>%
    mutate(win = if_else(position == 1, 1, 0)) %>% 
    group_by(sample_id) %>% 
    summarise(accuracy = mean(win))

hr_random_accuracy %>% 
    ggplot() +
    geom_histogram(aes(accuracy), binwidth = .001) +
    geom_vline(aes(xintercept = mean(accuracy))) +
    geom_label(aes(mean(accuracy), 1, label = round(mean(accuracy), 3)))
```
```{r, include=FALSE}
random_accuracy_percent <- mean(hr_random_accuracy$accuracy)

random_accuracy_ninefive <- hr_random_accuracy$accuracy %>% 
    quantile(probs = c(.025, .975)) %>%
    percent() %>% 
    glue_collapse(sep = ' - ')
```

So it's an mean accuracy of `r percent(random_accuracy_percent)`, with 95% range of [`r random_accuracy_ninefive`]. That's about a 1 in `r round(1/random_accuracy_percent)` chance of picking the winning horse. At first I thought this was a little low, as the average number of horses in a race was about 6, so I naively assumed that the random method would give us a 1 in 6 chance of picking the winnow, or `r percent(1/6)` accuracy level. But this assumption assumes a uniform probability of winning for each horse, which of course is not correct.

Accuracy is one thing, but what about our returns? Let's take a look at the at the cumulative return over time and its distribution. 

```{r, message=FALSE}
hr_random %>% 
    filter(sample_id %in% 1:40) %>% 
    ggplot() +
    geom_line(aes(sample_race_index, cumulative.ppr, group = sample_id), alpha = .5) +
    labs(
        title = "Dollar Bet - Random Horse per Race",
        subtitle = 'Cumulative Profit per Race',
        x = 'Race Index',
        y = 'Dollars'
    )
hr_random %>% 
    group_by(sample_id) %>% 
    summarise(total_ppr = sum(bet.profit / n())) %>%
    ggplot() +
    geom_histogram(aes(total_ppr), binwidth = .01) +
    geom_vline(aes(xintercept = mean(total_ppr))) +
    labs(
        title = 'Dollar Bet - Random Horse - Profit per Race Distribution',
        subtitle = 'Bin Width = .01',
        x = 'Total Profit',
        y = 'Count'
    )
```

```{r include = FALSE}
random_sample_profits <- hr_random %>%
    group_by(sample_id) %>% 
    summarise(ppr = sum(bet.profit) / n()) %>%
    pull(ppr)

random_mean_profit <- mean(random_sample_profits)

ninefive_percent_range <- random_sample_profits %>%
    quantile(probs = c(.025, .975)) %>%
    dollar() %>%
    escape_latex() %>% 
    glue_collapse(sep = ' - ')
```

The result isn't great: in the long run we're definitely losing money. You can see some occasional big jumps where we've managed to pick the long shot and pull our PPR into positive terriroty, but over time we trend back down into the red. In the long run our average profit per race `r escape_latex(dollar(random_mean_profit))` per race, and 95% of profits per race are in the range of [`r ninefive_percent_range`].  

# Approach 2 - Favourite

The second approach to take is to bet on the favourite in each race. We rank each horse in each race using the `order()` function, and extract the horse with a rank of 1. For races where there are two equal favourites, we pick one of those horses at random.

```{r}
# Favourite horse from each race
hr_favourite <- hr_mccv %>% 
    drop_na(odds.sp) %>% 
    group_by(sample_id, race_id) %>% 
    mutate(odds.rank = order(odds.sp)) %>% 
    slice_min(odds.rank, with_ties = FALSE, n = 1) %>% 
    ungroup()
    
# Place out bets
hr_favourite <- bet_ppr(hr_favourite)
```

What's our accuracy look like for this kind of bet?

```{r, message=FALSE}
# Calculate the accuracy
hr_favourite_accuracy <- 
    hr_favourite %>%
    mutate(win = if_else(position == 1, 1, 0)) %>% 
    group_by(sample_id) %>% 
    summarise(accuracy = mean(win))

# Graph
hr_favourite_accuracy %>%  
    ggplot() +
    geom_histogram(aes(accuracy), binwidth = .001) +
    geom_vline(aes(xintercept = mean(accuracy))) +
    geom_label(aes(mean(accuracy), 1, label = round(mean(accuracy), 3)))
```
```{r, include=FALSE}
# Accuracy for printing in paragraph below
favourite_accuracy_percent <- mean(hr_favourite_accuracy$accuracy)

favourite_accuracy_ninefive <- hr_favourite_accuracy$accuracy %>% 
    quantile(probs = c(.025, .975)) %>%
    percent() %>% 
    glue_collapse(sep = ' - ')
```
i
This is looking much better - we've got a mean accuracy across all of the samples of `r percent(favourite_accuracy_percent)` wih a 95% range of `r favourite_accuracy_ninefive`. These accuracy percentages look pretty good, and gut feel is that they would be pretty difficult to even get close to with any sort of predictive model. Picking the favourite is around `r round(favourite_accuracy_percent / random_accuracy_percent * 100)` times better than when picking a random horse.

Let's take a look at the cumulative returns over time:

```{r, message=FALSE}
hr_favourite %>%
    filter(sample_id %in% 1:40) %>% 
    ggplot() +
    geom_line(aes(sample_race_index, cumulative.ppr, group = sample_id), alpha  = .5) +
    labs(
        title = "Dollar Bet - Favourite - Cumulative Profits per Race",
        x = 'Race Index',
        y = 'Dollars'
    )

hr_favourite %>% 
    group_by(sample_id) %>% 
    summarise(profit = sum(bet.profit)) %>%
    ggplot() +
    geom_histogram(aes(profit), binwidth = 5) +
    geom_vline(aes(xintercept = mean(profit))) +
    labs(
        title = 'Dollar Bet - Favourite - Total Profit Counts',
        subtitle = 'Bin Width = 5',
        x = 'Total Profit',
        y = 'Count'
    )
```

```{r include = FALSE}
favourite_sample_profit <- hr_favourite %>%
    group_by(sample_id) %>% 
    summarise(ppr = sum(bet.profit) / n()) %>%
    pull(ppr)

favourite_mean_profit <- mean(favourite_sample_profit)

ninefive_percent_range <- favourite_sample_profit %>%
    quantile(probs = c(.025, .975)) %>%
    dollar() %>%
    escape_latex() %>% 
    glue_collapse(sep = ' - ')
```

This is much better than picking a random horse, but it's certainly no slam dunk. We've got longer stretches with a positive return on our investment, but again in the long run our PPR trends to negative. The mean PPR is `r dollar(favourite_mean_profit)`, with the 95% of PPRs in the range of [`r ninefive_percent_range`].

# Conclusion 

In this article we baselined two different approaches to betting on horse races: picking a random horse, and picking the favourite. Our aim was determine the mean accuracy , and the profits per race, for each of these approaches.

We found the accuracy of picking a random horse is `r percent(random_accuracy_percent)` and the mean profits per race for a dollar bet are `r dollar(random_mean_profit)`.

Betting of the favourite is of course markedly better, with a mean accuracy of %`r percent(favourite_accuracy_percent)`, however the mean profits per race for a dollar bet are `r dollar(favourite_mean_profit)`, so betting on the favourite does not guarantee us a profit. This makes sense: if this method of betting did guarantee us a profit, everyone would be doing it and the bookies would go out of business.

What we don't take into account here is the utility, or enjoyment, that is gained from the bet. If you think cost of the enjoyment you receive  betting on a random horse is worth around `r percent(abs(random_mean_profit))` of your stake, or betting on the favourite is worth `r percent(abs(favourite_mean_profit))` of your stake, then go for it. As long as you're not betting more than you can afford, then I say analyses be damned and simply enjoy the thrill of the punt.





