---
title: Honest Insurance Company
author: Greg Foletta
date: '2023-09-28'
slug: []
categories: [R Bayesian]
tags: []
---

```{r include=FALSE}
library(tidyverse)
library(tidybayes)
library(RSelenium)
library(rvest)
library(glue)
library(cmdstanr)
library(here)
library(tidybayes)
library(bayesplot)
library(gganimate)
library(scales)
library(ggdag)
library(gt)
library(plotly)
```

Last month my car - a Toyota Kluger - was run into while parked in front of my house. Luckily no one was injured and while annoying, the person had insurance (coincidently with the same company as my). The insurance company came back and determined that the car had been written off and I would be paid out the market value of the car. But what is the market value? How could I keep the insurance company honest and make sure I wasn't getting stiffed?

In this post I'll go through the process I used to keep the insurance company honest. There's data acquisition and visiualisation of the current market, then modelling of the market price. 

# TL;DR

A word of warning: this article is slightly anticlimactic. While we do find with, using Bayesian modelling, that the sale price of a Toyota Kluger will reduce by around .6% for every 1,000 km driven, it's not the whole story. There's a lot of other factors that need to taken into account. But it's better to start off with a simple model, assess, and slowly increase the complexity. So it's not the end, it's really just the beginning.

# Data Aquisition

The first step was to acquire some data on the current market for Toyota Klugers. A small distinction is that data will be the *for sale* price of the car, rather than the *sold* price, but I'm hoping it still provides a good representaton of the market.

We'll pull the data from a site that advertises cars for sale. The site requires Javascript to render, so a simple HTTP GET of the site won't work. Instead we need to render the page in a browser. We'll use a docker instance of the webdriver [Selenium](https://www.selenium.dev/), interfacing into this with the R package [RSelenium](https://github.com/ropensci/RSelenium) to achieve this. This allows us to browse to the site from a 'remotely controller' browser, Javascript and all, and retrieve the information we need.

We connect to the docker instance, setting the page load strategy to eager. This will speed up the process as we won't be waiting for stylesheets, images, etc to load.

```{sh eval=FALSE, include=FALSE}
# Stop running containers
if [ $(docker container ls -q --filter name=rsel --all) ]
then
        docker container stop rsel
        docker container rm rsel
fi
```

```{sh eval=FALSE, include=FALSE }
docker run -d -p 4444:4444 --name rsel selenium/standalone-firefox:latest
```


```{r eval = FALSE, echo=TRUE, results = 'hide'}
rs <- remoteDriver(remoteServerAddr = '172.17.0.2', port = 4444L)
rs$extraCapabilities$pageLoadStrategy <- "eager"
rs$open()
```

Each page of Klugers for sale is determined by an offset of 12. We generate the offsets (12, 24, 36 etc) and the URIs based on these offsets. We then navigate to each page, reading the source, and parsing into a structuered XML document.

```{r include=FALSE}
car_site_uri <- "https://www.carsales.com.au"
```

```{r eval = FALSE}
kluger_source <-
    tibble(
        # Generate offsets
        offset = 12 * c(0:100),
        # Create URIs based on offsets
        uri = glue("{car_site_uri}/cars/used/toyota/kluger/?offset={offset}")
    ) |> 
    mutate(
        # Naviate to each URI, read and parse the source
        source = map(uri, ~{ 
            rs$navigate(uri)
            rs$getPageSource() |> pluck(1) |> read_html()
        } )
    )
```

With the raw source in our hands, we can move on to extracting the pieces of data we need from each of them.

# Data Extractiion

First up, we define a small helper function which finds an element based on its XPath, and pulls out the text of that element.

```{r}
# XPath helper function, xpt short for xpath_text
xpt <- function(html, xpath) {
    html_elements(html, xpath = xpath) |> 
    html_text()
}
```

Each 'card' has the details of a car for sale. The issue we ran into is that not all of them have the odometer reading, which is the critical variable we're going to use in our modelling later. To get around this, we use a some convoluted XPath. We find all the <li> tags that have the odometer reading, then go back up the tree to find the ancestor <div> tags that define the entire card. This ensures that all the cards we've pulled out have odometer readings. 

From there, it's trivial to extract specific properties from the car sale.

```{r eval = FALSE}
kluger_data <-
    kluger_source |> 
    mutate(
        # Get entires that have odometer
        cards = map(source, ~html_elements(.x, xpath = "//li[@data-type = 'Odometer']/ancestor::div[@class = 'card-body']")),
        # Extract specific values of each car sale
        price = map(cards, ~xpt(.x, xpath = ".//a[@data-webm-clickvalue = 'sv-price']")),
        title = map(cards, ~xpt(.x, xpath = ".//a[@data-webm-clickvalue = 'sv-title']")),
        odometer = map(cards, ~xpt(.x, xpath = ".//li[@data-type = 'Odometer']")),
        body = map(cards, ~xpt(.x, xpath = ".//li[@data-type = 'Body Style']")),
        transmission = map(cards, ~xpt(.x, xpath = ".//li[@data-type = 'Transmission']")),
        engine = map(cards, ~xpt(.x, xpath = ".//li[@data-type = 'Engine']"))
    ) |>
    select(-c(source, cards, offset)) |>
    unnest(everything())
```

```{r include=FALSE}
kluger_data <- 
    read_csv('data/carsales_kluger.csv') |> 
    select(-offset)
```

At this stage, the data is a bit raw: the odometer and price are character strings with dollar signs and commas, and other important pieces of info are in the title:

```{r echo=FALSE, comment=""}
slice_sample(kluger_data, n = 50) |>
    gt() |>
    fmt_number(decimals = 2) |> 
    tab_header("Kluger Market Data") |>
    cols_width(title ~ px(300)) |> 
    tab_style(
        style = cell_text(size = 10),
        locations = cells_body(everything())
    ) |>  
    opt_interactive(
        use_text_wrapping = FALSE,
        use_highlight = TRUE,
        use_resizers = TRUE,
        use_compact_mode = TRUE,
        page_size_default = 5
    )
```

There's a small amount of housekeeping to be done. The price and odometer are in a textual format, so these are converted to integers. we also create a new *megametre* variable (i.e. thousands of kilometers). The year, model, and drivetrain are pulled out of the title of the advert using regex.

```{r, comment=''}
kluger_data <-
kluger_data |>
    mutate(
        odometer = parse_number(odometer),
        odometer_Mm = odometer / 1000,
        price = parse_number(price),
        year = as.integer( str_extract(title, "^(\\d{4})", group = TRUE) ),
        drivetrain = str_extract(title, "\\w+$"),
        model = str_extract(title, "Toyota Kluger ([-\\w]+)", group = TRUE)
    )
```

```{r echo=FALSE}
slice_sample(kluger_data, n = 200) |>
    select(title, model, year, price, odometer, drivetrain, model) |> 
    gt() |>
    fmt_number(decimals = 0) |>
    fmt_integer(year, use_seps = FALSE) |> 
    tab_header("Kluger Market Data") |>
    cols_width(title ~ px(300)) |> 
    tab_style(
        style = cell_text(size = 10),
        locations = cells_body(everything())
    ) |>  
    opt_interactive(
        use_text_wrapping = FALSE,
        use_highlight = TRUE,
        use_resizers = TRUE,
        use_compact_mode = TRUE,
        page_size_default = 5
    )
```
# Taking a Quick Look

Let's visualise key features of the data. First up we'll, how does the market price for a Kluger change as the odometers (in megametres):

```{r echo = FALSE}
kluger_data |>
    ggplot() +
    geom_point(aes(odometer_Mm, price, colour = model), alpha = .5) +
    labs(
        title = "Market for Toyota Klugers",
        subtitle = "Odometer Versus Price",
        x = "Odometer (megametres)",
        y = "Price ($)",
        colour = "Model"
    ) + 
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The thing I notice is that looks suspiciously like there's some sort of negative exponential relationship between the the odometer and price. What if we take a look at the odometer versus the log of the price?

```{r echo = FALSE}
kluger_data |>
    ggplot() +
    geom_point(aes(odometer_Mm, log(price), colour = model)) +
    labs(
        title = "Toyota Kluger Market",
        subtitle = "Odometer Versus Log(Price)",
        x = "Odometer (megametres)",
        y = "Log(Price) ($)"
    ) +
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
There's some good news, and some bad news here. With the log transform we've now got a linear relationship between the number odometer of the car and the price. This is going to allow us to fit a nice, simple linear model to the data.

The bad news is that the data varies significantly (heteroskedacticity) across the odometer ranges. This won't affect our linear model, but will likely affect the prediction capability of our model. We'll persevere nonetheless.

The log transform provides a nice interpretation for the slope of this line. Recall that in general when you fit a line to x and y, the slope (\\(beta\\)) of that line is "the change in the y variable given a change of one unit of the x variable". When you fit a line to to x and log(y) (called log-linear), for small \\(\beta\\), \\(e^\beta\\) is the percentage change in y for a one unit change of x. 

Here's the same view, but we split it out by model:

```{r echo=FALSE}
kluger_data |>
    ggplot() +
    geom_point(aes(odometer_Mm, log(price), colour = model), alpha = .3) +
    facet_wrap(~model) +
    labs(
        title = "Toyota Kluger Market",
        subtitle = "Odometer versus Log(Price) by Model",
        x = "Odometer (megametres)",
        y = "Log(Price) ($)"
    ) +
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Modelling

Let's start by thinking about the generative model of the price. Our observed variables odometer, year, model, and drivetrain are likely going to have an affect on price. There are some unobserved variables, such as the condition of the car its popularity that would also have an affect. There's some [confounds](https://en.wikipedia.org/wiki/Confounding) that may need to be dealt with as well: year directly affects price, but also affects through the odometer (older cards are more likely to have more kilometres). Model affects price, but also affects through the drivetrain (certain models have certain drivetrains).

The best way to visualise this is using the directed acyclic graph (DAG):

```{r echo=FALSE}
dag_coords <- tribble(
    ~name,    ~x,    ~y, 
    "price",   0,     0,
    "odom.",   1,    -1,
    "year",    1,     0,
    "model",   1,     1,
    "drvtrn",  1,     2,
    "cond.",   -1,    1,
    "pop.,",   -1,    -1 
)
    
generative_dag <- 
    dagify(
        price ~ odom. + year + model + cond. + drvtrn + pop.,
        drvtrn ~ model,
        odom. ~ year
        #coords = dag_coords
    ) |> 
    tidy_dagitty() |>
    mutate(
        type = case_when(
            name == 'price' ~ "Outcome",
            name %in% c('cond.', 'pop.') ~ "Unobserved",
            .default = "Observed"
        )
    )

generative_dag |> 
    ggdag() +
    geom_dag_point(aes(colour = type)) +
    geom_dag_text() +
    theme_dag_grey() +
    labs(
        title = "Toyota Kluger Pricing Generative Model",
        subtitle = "Directed Acyclic Graph",
        colour = "Variable Type"
    )
```
While I do have these variables available to me, I'd like to start with the most simple modelcan: log of the price predicted by the odometer (in megametres). In doing this I'm leaving a lot of variability on the table, so the model's ability to predict is likely going to be hampered. But better to start simple a build up (also leaves the door open for follow-up articles).

At this point I could rip our a standard linear regression, get the coefficients and *bang* we're done. But where's the sport in that? Instead, I'll use this as an opportunity to model this in a Bayesian manner.

# Bayesian Modeling

I'm going to be using [Stan](https://mc-stan.org/) to perform the statistical modelling, executing it from R using the [cmdstanr](https://mc-stan.org/cmdstanr/) package. Here's the program I've written:


```{r include=FALSE}
model_file_path <- here('content', 'post', '2023-09-28-honest-insurance-company', 'linear.stan')
```

```{r echo=FALSE, comment=''}
kluger_model <- cmdstan_model(model_file_path)
kluger_model$print()
```
It should be relatively easy to read: the data is our observed odometer (in megametres) and price, the parameters we're looking to find are *a* (for alpha, the intercept), *b* (for beta, the slope), and *sigma* (our variance). I'm pretty sure the slope is going

```{r eval=FALSE, include=FALSE}
tibble(
    a = rnorm( 100, log(60000), 1),
    b = runif( 100, log(.98), log(1) ) 
    ) |>
    ggplot() +
    geom_abline(aes(intercept = a, slope = b)) +
    geom_point(data = kluger_data, aes(x = odometer_Mm, y = log(price)))
```


The generated values helps us with prediction. For each of our *n* ovbservations the model is run with the parameters drawn out of the posterior distribution and the results stored in *y_s*. This allows us to compare what our model thinks is are resonable outcomes and help us determine how well the model performs. The *price_pred* is the prediction at 60,000km, which was the reading on the odometer of my car that was written off. 

Let's run the model with the data:

```{r echo=TRUE, comment=''}
kluger_fit <- kluger_model$sample(
    data = compose_data(kluger_data),
    seed = 123,
    chains = 4,
    parallel_chains = 4,
    refresh = 500,
)
```

# Assessing the Model

We want to take a look at the resulting parameters and make sure they're reasonable. First up is a histogram of the posterior distributions of each of the parameters. 

```{r echo = FALSE}
kluger_fit |> 
    gather_draws(a, b, sigma) |>
    recover_types() |> 
    ggplot() +
    geom_histogram(aes(.value, fill = as.factor(.chain)), bins = 100) +
    facet_wrap(vars(.variable), scales = 'free') +
    labs(
        title = "Toyota Kluger Market Linear Model",
        subtitle = "Histogram of Posterior Draws of Alpha & Beta Coefficients",
        x = "Coefficient Value",
        y = "Frequency",
        fill = "Chain"
    )
```
This looks good, each looks Gaussian in shape, and each of the chains has a similar view of the posterior distribution. A related check is the *trace plot*. We want these to look like "fuzzy caterpillars", shows that each chain is exploring the distribution in a similar way, and isn't wandering off on its own for too long.
```{r echo=FALSE}
kluger_fit |> 
    gather_draws(a,b, sigma) |>
    ggplot() +
    geom_line(aes(.iteration, .value, colour = as_factor(.chain)), alpha = .8) +
    facet_grid(vars(.variable), scales = 'free_y') + 
    labs(
        title = "Toyota Kluger Market Model",
        subtitle = "MCMC Trace Plot",
        x = "Iteration",
        y = "Value",
        colour = "Chain"
    )
```
So we're reasonably confident that all of the chains have converged, let's take a look at how the distribution of parameters looks over our observations. We're not dealing with point estimates as we would with a linear regression. Instead we've got a distribution of plausible intercepts and slopes. One way to visualise this is to plot each line from the distribution, colouring it with its distance from the mean.

```{r echo = FALSE}
#fit_mean_a <- kluger_fit$summary()$mean[2]
#fit_mean_b <- kluger_fit$summary()$mean[3]
kluger_param_quantile <-
kluger_fit |> 
    spread_draws(b) |>
    reframe(
        interval = c(.055, .945),
        value = quantile(b, interval)
    ) |>
    mutate(exp_value_pct = (1 - exp(value)) * 100)

kluger_fit_animation <-
    kluger_fit |>
    spread_draws(a, b) |>
    recover_types() |>
    mutate(abs_mean_distance = abs(b - mean(b))) |> 
    ggplot() +
    geom_point(data = kluger_data, aes(odometer_Mm, log(price))) +
    geom_abline(aes(intercept = a, slope = b, group = .draw, colour = abs_mean_distance), linewidth = .1) +
    transition_reveal(.draw) +
    guides(colour = guide_legend(override.aes = list(alpha = 1))) +
    labs(
        title = "Toyota Kluger Market Model",
        subtitle = "Odometer vs Log(Price) with MCMC Coefficients (Draw {frame_along})",
        x = "Odometer (Megametres)",
        y = "Log(Price) ($)",
        colour = "Absolute Distance from Mean"
    ) +
    scale_x_continuous(labels = scales::comma) +
    scale_y_continuous(labels = scales::comma) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
 
animate(kluger_fit_animation, )
```
But plotting each of the draws from the posterior distribution, what we end up getting is a view of the confidence interval of the intercept and slope parameters. The good news here is that the distribution is not very wide (which we saw in the histograms above). Looking at the 89% interval of the slope parameter we see it's between `r kluger_param_quantile[['value']][[2]]` and `r kluger_param_quantile[['value']][[1]]`. Exponentiating this and turning into percentages, we find that, on average, for every 1,000km on the odometer, the sale price of a Toyota Kluger decreases by between `r percent(1-exp(kluger_param_quantile[['value']][[2]]), accuracy = .001)` and `r percent(1-exp(kluger_param_quantile[['value']][[1]]), accuracy = .001)`.

The bad news is news we really already knew - the variance around our line is very large and it changes as the odometer value changes. A check for this is posterior prediction, which uses the *generated values* section from our Stan program. That section generated log(price) values based on the linear model, using parameters pulled from the posterior distribution. What we want to see is, using random data, how close to the real observations is our generated model?

```{r echo=FALSE}
kluger_fit |>
    recover_types() |>
    spread_draws(y_s[i]) |>
    sample_frac(size = .1) |>
    select(-c(.chain, .iteration, .draw)) |>
    nest(y_s = y_s) |>
    bind_cols(kluger_data) |>
    unnest(y_s) |>
    ggplot() +
    geom_point(aes(odometer_Mm, y_s), colour = 'lightblue', alpha = .05) +
    geom_point(data = kluger_data, aes(odometer_Mm, log(price)), alpha = .5) +
    labs(
        title = "Toyota Kluger Market Model",
        subtitle = "Odometer vs Log(Price) with Postierior Predictions", 
        x = "Odometer (Megametres)",
        y = "Log(Price) ($)"
    )
```
So while our parmaeters for the linear model look good, the linear model is not great. At odometer values close to zero it's too conservative, with the all the prices falling well inside our prediced bands in light blue. At the other end of the scale the model is too conservative, with many of the real overservations falling outside of our predictive bands.

Despite this, let's see where we ended up with the question at hand: "what is the market sell price for a Toyota Kluger with 60,000kms on the odometer?" We use the results of the *generated values* section, with parameters drawn from the posterior distribution but this time fixing the odometer reading at 60 megametres. Here's the resulting distribution of prices with an 89% confidence interval (5.5% and 94.5% quaniles):

```{r echo=FALSE}
kluger_quantile <-
    kluger_fit |>
    spread_draws(price_pred) |>
    reframe(
        interval = c(.055, .945),
        value = quantile(price_pred, interval)
    ) |>
    spread(interval, value)

kluger_fit |>
    recover_types() |>
    spread_draws(price_pred) |>
    ggplot() +
    geom_histogram(aes(price_pred), bins = 200) +
    geom_vline(xintercept = kluger_quantile[['0.055']], color = 'blue', linewidth = 1, linetype = 'dotted') +
    geom_vline(xintercept = kluger_quantile[['0.945']], color = 'blue', linewidth = 1, linetype = 'dotted') +
    scale_x_continuous(labels = scales::dollar) +
    scale_y_continuous(labels = scales::comma) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    labs(
        x = "Count",
        y = "Predicted Price ($)",
        title = "Toyota Kluger Price Prediction",
        subtitle = "Distribution of Price Prediction at 60,000km with 89% Quantiles"
    )
```
That's a large spread, with an 89% interval between `r kluger_quantile[[1]] |> dollar()` and `r kluger_quantile[[2]] |> dollar()`. That's too large to be of any use to us in validating the market value the insurance company gave me for my car.

# Summary

```{sh eval=FALSE, include=FALSE}
docker container stop rsel
docker container rm rsel
```

