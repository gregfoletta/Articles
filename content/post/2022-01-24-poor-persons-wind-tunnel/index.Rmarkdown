---
title: Poor Man's Wind Tunnel
author: ''
date: '2021-12-24'
slug: 
categories: []
tags: []
images: []
---

```{r include=FALSE}
library(tidyverse)
library(broom)
library(lubridate)
library(xml2)
library(scales)
```

I raced bikes as a junior and then came back to the sport after a twenty year hiatus. One of the biggest contrasts I've seen is the affordability of bike sensors. All I used to have 'back in the day' was a simple computer that told me my speed and cadence. Now I've got that, plus position via GPS, power, heart rate, pedal smoothness, with the data collected and presented on  my iPhone.

Collecting and viewing this data is all well and good, but for the longest time I've been wanting to do something more with it. I finally had the idea to use the data to try and determine the aerodynamics of different positions on the bike. So in this article we're going to attempt to answer to the following question:

> How much more aerodynamically efficient is it to ride with your hands on the drops of the handlebars, rather than the tops?

There are two main sections of the article: in the first section we look at data was generated, and then how we load and transform the data to get it ready for analysis. It's in this section where we see R really shine, with a simple and element method of transforming XML into a rectangular, tidy data format. In the second section we define an aerodynamic model (or more accurately reuse a common model), then perform a simple regression of this data to determine the model parameters. 

# Data Acquisition

Let's first talk about how the experimental set up and how the data was captured. I used a track bike (which has a single fixed gear) and rode it around the [Coburg velodrome](https://www.google.com/maps/@-37.7297305,144.9553304,147m/data=!3m1!1e3) which is a 250m outdoor track. A sensor (Wahoo speed) on the hub of the wheel collected speed data, my pedals (PowerTap P1) collected power and cadence, and a strap around my chest collected my heart rate.

Data was gathered while in two different positions on the bike[^1]. The first we will call on the 'tops', looked like this:
[^1]: Images courtesy of [bikegremlin.com](http://bikegremlin.com)

```{r, echo=FALSE, fig.align="center", out.extra='style="width:40%;height:40%;"'}
knitr::include_graphics("tops.jpg")
```

The second which we will call the 'drops', looked like this:

```{r, echo=FALSE, fig.align="center", out.extra='style="width:40%;height:40%;"'}
knitr::include_graphics("drops.jpg")
```

For each position the pace was slowly increasing from 10km/h to to 50km/h, in 8-10km/h increments. For each increment level, the pace was held as close as possible to constant for two laps, increasing to three laps for higher speeds in order to get enough samples.

There are two main external elements which affect our data generation process: wind, and the fact that the Coburg velodrome is not completely flat. However because we are moving around and oval, some noise will be added but none of the data should be biased. This noise will simply increase our uncertainty about our model's parameters.

# Transforming the Data

The data is downloaded in TCX (Training Center XML) format. While good for us that it's in a standard structured format, it's not quite in the rectangular tidy data structure that we need for our analysis. Our first step is to extract and transform it into this format. The XML is structured as a single *activity* with one or more *laps*. Each *lap* has *trackpoints* which contain a timestamp and the other data (speed, power, heartrate, etc) that's been ollected. A trackpoint is taken every one second.

The full file is available [here](cycle_data.tcx), but here's an example of the XML from the root to the a trackpoint. Only one lap and one trackpoint is shown.

```xml
<TrainingCenterDatabase>
    <Activities>
        <Activity>
            <Lap>
                <Track>
                    <Trackpoint>
                        <Time>2022-01-16T00:00:41Z</Time>
                        <DistanceMeters>1.48</DistanceMeters>
                        <HeartRateBpm>
                            <Value>105</Value>
                        </HearthRateBpm>
                        <Cadence>32</Cadence>
                        <Extensions>
                            <TPX>
                                <Speed>3.19</Speed>
                                <Watts>56</Watts>
                            </TPX>
                        </Extensions>
                    </Trackpoint>
                    <!-- Multiple trackpoints (1 second per sample) --> 
                </Track>
            </Lap>
            <!-- Multiple laps (generated manually) -->
        </Activity>
    </Activities>
</TrainingCenterDatabase>
```

In what I think is a great example of the elegance and power of R, the following code takes our TCX file and uses XPath to extract out the fields we need, turning it into a tidy data frame.

```{r, comment=""}
cycle_data <-
    read_xml('cycle_data.tcx') %>%
    xml_ns_strip() %>%
    xml_find_all('.//Trackpoint[Extensions]') %>%
    {
        tibble(
            time = xml_find_first(., './Time') %>% xml_text() %>% ymd_hms(),
            speed = xml_find_first(., './Extensions/TPX/Speed') %>% xml_double(),
            power = xml_find_first(., './Extensions/TPX/Watts') %>% xml_integer(),
            bpm = xml_find_first(., './HeartRateBpm/Value') %>% xml_integer(),
            cadence = xml_find_first(., './Cadence') %>% xml_integer(),
            lap = xml_find_num(
                .,
                'count(./parent::Track/parent::Lap/preceding-sibling::Lap)'
            ),
        )
    }
```

```{r, echo = FALSE}
head(cycle_data, n = 10) %>% 
    knitr::kable()
```

I think it's worth going through each line:

1. The TCX file is read in as as an *xml_document*
1. The TCX is namespaced, but as we're only working with this file we strip the namespace to make our XPath shorter.
1. Using the `.//Trackpoint[Extensions]` XPath We find all trackpoint nodes that have a child extensions node. We can't just find all of the trackpoints as there seems to be a quirk where the first trackpoint has a timestamp but no other data.
1. We then construct a data frame (a tibble) by finding and extracting the text from our data nodes (speed, power, etc) from each of the trackpoints. The pipe to tibble is enclosed in braces to stop the left-hand side from automatically being placed as the first argument. 
1. To determine which lap the trackpoint is part of we find it's grandparent lap node, and then count how many preceding lap siblings it has. So the first lap has 0 preceding siblings, the second lap has 1, etc.

And that's it! with less than 20 lines of code we've been able to transform our XML into a tidy, rectangular data format ready for visualisation and analysis. Speaking of visualisation, let's take a look at a few different aspects of the data to get a general feel for it. First off is the power output over time with each lap coloured separately. Laps one and three contain the data that we will be using in our model.

```{r echo=FALSE}
cycle_data %>% 
    ggplot() +
    geom_line(aes(time, power, color = as.factor(lap)), size = .4) +
    labs(
        title = 'Track Cycling - Power over Time',
        subtitle = 'Laps differentiated by colour',
        x = 'Time',
        y = 'Power (Watts)',
        colour = 'Lap'
    )
```

As the the data was generated on a track bike which has only a single gear, the speed and cadence should have a near perfect linear relationship.

```{r echo=FALSE}
cycle_data %>% 
    ggplot() +
    geom_point(aes(cadence, speed), size = .4) +
    labs(
        title = 'Speed versus Cadence',
        subtitle = 'Data generated on a single-gear track bike',
        x = 'Cadence (RPM)',
        y = 'Speed (Metres/Second)'
    )
```

We see the linear relationship but note that there is a distribution of speeds across each cadence value. This is likely due to the difference in precision between the cadence and the speed, as cadence is measured as a integer whereas speed is a double with a single decimal point[^2].

[^2]: A linear regression of cadence on speed was performed, and the residuals were in the range of (-.5, .5). This supports our precision difference hypothesis.

Finally, let's take  look at the data we'll be modelling and its relationship. We extract out the second and fourth laps from the data, then create a new *position* factor variable with appropriately named levels. We're also going to remove data where we were accelerating - i.e. the rate of change of the power between trackpoint samples was between -20 and 20 watts. I had to accelerate to move to different speed increments, but our model only relates to points of (relatively) constant speed and so these aren't valid for our model.


```{r}
cycle_data_cleaned <-
    cycle_data %>% 
    filter(
        lap %in% c(1,3),
        between(power - lag(power), -10, 10)
    ) %>%
    mutate(position = fct_recode(as_factor(lap), "Tops" = "1", "Drops" = "3"))
```

I should note that removing data to fit a model is not generally something that should be done. The difference here is that I was in control of the the data generation (not just data acquisition) process, and I have justified the rationale for the removal of the data. 

We can now view the power output versus the speed of this cleaned data.

```{r echo=FALSE}
cycle_data_cleaned %>% 
    ggplot() +
    geom_point(aes(speed, power, colour = position), size = .8) +
    labs(
        x = 'Speed (Metres/Second)',
        y = 'Power (Watts)',
        title = 'Speed versus Power',
        colour = 'Hand Position'
    )
```

We see some sort of exponential relationship between speed and power (we'll discuss that in the next section). We can also see the "blobs" of data where I have tried to keep a constant speed, and how keeping that constant speed become more difficult as I went faster. What is not instantly visible is the difference in power output versus speed for each of the different hand positions.


# Defining and Building a Model

Before we build our model in R we first have to define what the model is going to be. We'll be using the going to be using the classic drag equation: 
y
$$ F_d = \frac{1}{2}\rho C_D A v^2$$
This says that the force of drag \\(F_d\\) on a body when moving through a fluid is proportional to half of the density of the fluid (\\(\rho\\)) times the drag coefficient of my bike/body (\\(C_D\\)) time  is front on cross-sectional area (\\(A\\)) times the square of my is my velocity (\\(v\\)). I'm going to bundle up all coefficients into a single coefficient \\(\beta\\).

$$ \text{Let } \beta = \frac{1}{2} \rho C_D A $$
$$ F_d = \beta v^2 $$
We've got force on our left-hand side, but we need power. Energy is force times distance, and power is energy over time, so we have:

$$ F_d \Big( \frac{x}{t} \Big) = \beta v^2 \Big( \frac{x}{t} \Big)$$ 
Distance over time is velocity so we are left with:

$$ P_d = \beta v^3 $$ 
Is this a perfect model? Not at all, but for our purposes it should be reasonable. Don't make me tap the "all models are wrong..." sign!

The model will give us an estimate (with some uncertainty) \\(\beta_{tops}\\) value when I was on the tops of the handlebars, and a \\(\beta_{drops}\\) value when I was in the drops.

We have some prior information that we can be included in the model: it takes zero watts to go zero metres per second. This implies that our model should go through the origin \\((0,0)\\) and we should not include an intercept. I believe that given our strong knowledge of the process that generated the data, removing the intercept is valid.

```{r}
cycle_data_mdl <-
    cycle_data_cleaned %>% 
    lm(power ~ 0 + position:I(speed^3), data = .) 
```

Here's what model looks like overlayed on the data:

```{r echo = FALSE}
crossing(
    speed = seq(0, 14, by = .1),
    position = as_factor(c('Tops', 'Drops'))
) %>%  
    mutate(
        power = predict(
            cycle_data_mdl,
            newdata = tibble(speed = speed, position = position)
        )
    ) %>%
    ggplot() +
    geom_line(aes(speed, power, colour = position)) +
    geom_point(
        aes(speed, power, colour = position),
        size = .4,
        data = cycle_data_cleaned,
    ) +
    labs(
        title = 'Speed over Power',
        subtitle = 'power ~ 0 + position x speed^3',
        x = 'Speed (Metres/Second)',
        y = 'Power (Watts)',
        colour = 'Hand Position'
    )
```

As expected the drops is more efficient that the tops. Before looking at the parameters of the model let's first look at some diagnostics. The first one to look at is a fitted value versus residual plot.

```{r echo=FALSE}
cycle_data_mdl %>%
    augment() %>%  
    ggplot(aes(.fitted, .std.resid)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = 'y~x') +
    labs(
        title = 'Fitted over Standardised Residuals',
        x = 'Fitted Value (Watts)',
        y = 'Standard Residual',
    )
```

This is very interesting! While we seem to have captured mos of the the variation in our model, there still seems to be a linear component that our model hasn't accounted for. If we think about this, our model is too simple and we've missed a critical component: friction! Let's re

# Building a Better Model

In the original model, \\(P_{Total} = P_{Drag}\\), but out **t**otal power used is made up of power to overcome **d**rag plus power to overcome **f**riction:

$$ P_{t} = P_{d} + P_{f} $$

Generally the force of friction is not proportional to the relative velocity of the two surfaces that are touching, but we're looking at power, so doing some conversion we end up with:

$$ P_{f} = \frac{ F_{f} \times x }{ t } = F_{f}v $$

If we let \\(\beta_1 = F_{f}) and from our previous model we let \\(\beta_2 = \beta), our model is now:

$$ P_{t} = \beta_1 v + \beta_2 v^3 $$
Let's re-do our modelling with this updated model. We don't beleive that the frictional component would be affected by the position on the handlebars, so we make sure it's not conditional on the position.

```{r}
cycle_data_mdl <-
    cycle_data_cleaned %>% 
    lm(power ~ 0 + speed + position:I(speed^3), data = .) 
```

Here's our updated on model on top of the original data:

```{r echo = FALSE}
crossing(
    speed = seq(0, 14, by = .1),
    position = as_factor(c('Tops', 'Drops'))
) %>%  
    mutate(
        power = predict(
            cycle_data_mdl,
            newdata = tibble(speed = speed, position = position)
        )
    ) %>%
    ggplot() +
    geom_line(aes(speed, power, colour = position)) +
    geom_point(
        aes(speed, power, colour = position),
        size = .4,
        data = cycle_data_cleaned,
    ) +
    labs(
        title = 'Speed over Power',
        subtitle = 'power ~ 0 + speed + position x speed^3',
        x = 'Speed (Metres/Second)',
        y = 'Power (Watts)',
        colour = 'Hand Position'
    )
```

Hard to discern if much difference from this graph, so let's look at the fitted versus residuals again:

```{r echo=FALSE}
cycle_data_mdl %>%
    augment() %>%  
    ggplot(aes(.fitted, .std.resid)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = 'y~x') +
    labs(
        title = 'Fitted over Standardised Residuals',
        x = 'Fitted Value (Watts)',
        y = 'Standard Residual',
    )
```
That's looking much better! We've now captured the linear component, the residuals are random, and the variation is reasonably even across the entire spread of fitted values. There are a few outliers, and a more rigourous analysis would look to determine whether they had significant leverage on our regression line. Subjectively looking at this graph though my guess would be no.

The other type of diagnostic we'll look at is a histogram of the residuals. A linear regression has an assumption that the residuals are normal. The residual shape doesn't affect the point estimates of the model, but does affect the confidence intervals of the parameters.

```{r echo=FALSE}
cycle_data_mdl %>%
    augment() %>%  
    ggplot() +
    geom_histogram(aes(.std.resid), binwidth = .2) +
    labs(
        title = 'Speed versus Power Model - Residual Histogram',
        subtitle = 'power ~ 0 + speed + position x speed^3',
        x = 'Standardised Residual (Bucket Size = .2)',
        y = 'Count'
    )
```

This looks great: the residuals have an approximate Gaussian shape, there's not much mass at more that 2 standard deviations, and the mean sits approximately at zero.

With confidence in the model we now take a look at the parameters:

```{r, echo = FALSE}
beta_1 <- tidy(cycle_data_mdl)[[2]][1]
beta_tops <- tidy(cycle_data_mdl)[[2]][2]
beta_drops <- tidy(cycle_data_mdl)[[2]][3]
percent_change <- ((beta_tops- beta_drops) / beta_tops) * 100

tidy(cycle_data_mdl) %>% 
    janitor::clean_names(case = 'title') %>% 
    knitr::kable()
```

The speed term is the \\(\beta_1\\) coefficient, which is the the frictional force of the bike. The model has determined that the frictional of the bike accounts for `r round(beta_1, 2)` Newtons of force.

The next two are the coefficients of the \\(v^3\\) term when the position varibale is 'Tops' and when it is 'Drops'. Value of the coefficient isn't important to us (being a combinatio of the fluid density, drag dofficient, and my cross-sectional area), but what we want to look at is the relative difference. The result is that, for a specific velocity, we need to use `r round(percent_change, 2)`% less power. Put another way, we are `r round(percent_change, 2)` more efficient in this position.

The following table gives you an idea on the differences in power required for speeds of 20, 40, and 60 km/h.


```{r echo=FALSE, comment=""}
crossing(
    kmph = c(20, 40, 60),
    position = as_factor(c('Tops', 'Drops')),
) %>% 
    mutate(
        speed = kmph / 3.6,
        power = round(predict(
            cycle_data_mdl,
            newdata = tibble(
                speed = speed,
                position = position
            )
        ), 2)
    ) %>% 
    pivot_wider(names_from = position, values_from = power) %>% 
    select(-speed) %>% 
    rename(Speed = kmph) %>% 
    mutate(`Power Difference` = Tops - Drops) %>% 
    knitr::kable()
```

# Summary

In this article we looked at the aerodynamics of different positions on a bike. We gathered data using different sensors, and showed the elegance of R by transforming XML data into a rectangular, tidy data frame.

We defined a simple model and used this to perform a regression of power required to maintain a specific velocity. By performaing diagnostics on this model, we were able to identify that our model was incomplete, and that we were likely not including friction in the model. We defined and created a new model with friction included, which performed better than our original model.

The ultimate aim of the article was to determine how much more efficient it is to ride in the 'drops' of the handlebars rather than the 'tops'. From our modelling we estimate it to be `r round(percent_change, 2)`% more efficient.


