---
title: Poor Person's Wind Tunnel (Draft)
author: 'Greg Foletta'
date: '2022-01-29'
categories: [R, Regression]
---

```{r include=FALSE}
library(tidyverse)
library(broom)
library(lubridate)
library(xml2)
library(scales)
```

I raced bikes as a junior and came back to it after a twenty year hiatus. One of the biggest contrasts I've seen in the sport is the proliferation of bike sensors. All I used to have 'back in the day' was a simple computer with speed and cadence. Now I've got that, plus position via GPS, power, heart rate, pedal smoothness and balance, all collected and displayed on my phone.

It's all well and good to collect and visualise this data, but surely there was more I could do with it? After much thought, I realised I could use it to determine my aerodynamic position on the bike. So in this article we're going to answer the question:

> How much more aerodynamically efficient is it to ride with your hands on the drops of the handlebars, rather than the tops?

There are two main sections of the article: in the first section we look at how the data was generated, loaded into R, and transformed into a state that's ready for analysis. It's in this section where we see R really shine, with a simple and element method of transforming XML into a rectangular, tidy data format. 

In the second section we define an aerodynamic model (or more accurately reuse a common model), then perform a simple regression of this data to determine the aerodynamic properties. Diagnostics on the model make us realise that we've missed a critical element, updating the model to get better estimates of the aerodynamic properties.

# Data Acquisition

We'll first look at how the experiment was set up and how the data was captured. A track bike (with has a single, fixed gear) was ridden around the [Coburg velodrome](https://www.google.com/maps/@-37.7297305,144.9553304,147m/data=!3m1!1e3) which is a 250m outdoor track. A sensor (Wahoo speed) on the hub of the wheel collected the speed, and the pedals (PowerTap P1s) collected power and cadence.

Data was gathered while in two different positions on the bike[^1]. The first position which we will call being on the 'tops' looked similar to this except without brake levers:
[^1]: Images courtesy of [bikegremlin.com](http://bikegremlin.com)

```{r, echo=FALSE, fig.align="center", out.extra='style="width:40%;height:40%;"'}
knitr::include_graphics("tops.jpg")
```

The second position which we will call being on the 'drops' looked like this:

```{r, echo=FALSE, fig.align="center", out.extra='style="width:40%;height:40%;"'}
knitr::include_graphics("drops.jpg")
```
For each position the pace was slowly increasing from 10km/h to to 45km/h in approximately 10km/h increments. For each increment level, the pace was held as close as possible to constant for two laps, increasing to three laps for higher speeds in order to get enough samples.

The experimental environemnt is far from clean, with two main external elements affecting our data generation process: wind, and the lumpyness of the velodrome. Because we are moving around and oval, both of these external elements will add noise to the data, but shouldn't bias it in any one direction. If there was any biasing effect it would be from wind gusts.

What do we expect from the experiment? We're expecting better aerodynamics when in the drops position due to two factors: a reduction in the front on surface area, and a more streamlined shape.

# Transforming the Data

The data is downloaded in TCX (Training Center XML) format. While good for us that it's in a standard structured format, it's not quite in the rectangular tidy data structure that we need for our analysis, so the first step is to extract and transform it. The XML is structured as a single *activity* with one or more *laps*. Each *lap* has *trackpoints* which contain a timestamp and the data collected (speed, power, heartrate, etc). A trackpoint is taken every one second.

The full file is available [here](cycle_data.tcx), but this is a high-level overview of the structure:

```xml
<TrainingCenterDatabase>
    <Activities>
        <Activity>
            <Lap>
                <Track>
                    <Trackpoint>
                        <Time>2022-01-16T00:00:41Z</Time>
                        <DistanceMeters>1.48</DistanceMeters>
                        <HeartRateBpm>
                            <Value>105</Value>
                        </HearthRateBpm>
                        <Cadence>32</Cadence>
                        <Extensions>
                            <TPX>
                                <Speed>3.19</Speed>
                                <Watts>56</Watts>
                            </TPX>
                        </Extensions>
                    </Trackpoint>
                    <!-- Multiple trackpoints (1 second per sample) --> 
                </Track>
            </Lap>
            <!-- Multiple laps (generated manually) -->
        </Activity>
    </Activities>
</TrainingCenterDatabase>
```

Thanks to the XML2 library, XPath queries, the vectorised nature of R, this is quite an easy task:

```{r, comment=""}
cycle_data <-
    read_xml('cycle_data.tcx') %>%
    xml_ns_strip() %>%
    xml_find_all('.//Trackpoint[Extensions]') %>%
    {
        tibble(
            time = xml_find_first(., './Time') %>% xml_text() %>% ymd_hms(),
            speed = xml_find_first(., './Extensions/TPX/Speed') %>% xml_double(),
            power = xml_find_first(., './Extensions/TPX/Watts') %>% xml_integer(),
            bpm = xml_find_first(., './HeartRateBpm/Value') %>% xml_integer(),
            cadence = xml_find_first(., './Cadence') %>% xml_integer(),
            lap = xml_find_num(
                .,
                'count(./parent::Track/parent::Lap/preceding-sibling::Lap)'
            ),
        )
    }
```

```{r, echo = FALSE}
head(cycle_data, n = 10) %>% 
    knitr::kable()
```

While terseness is elegant it can also make the code difficult to interpret, so I think it's valuable to go through each step of the pipeline:

1. The TCX file is read in as as an *xml_document*
1. The XML is namespaced, but as we're only working with this file we strip the namespace to make our XPath easier to work with.
1. Using the `.//Trackpoint[Extensions]` XPath we find all 'trackpoint' nodes that have a child 'extensions' node. 
    - We do this because some of the trackpoints only have a timestamp with no data.
1. We then construct a data frame (a tibble) by finding and extracting the speed, power, etc from each trachpoint, with the XPaths being relative to the trackpoint node.
    - The braces to stop the normal behaviour of the left-hand side of the pipe being passed as the first argument to the tibble.
1. Determining which 'lap' a trackpoint belongs to takes a little more work. We do this by finding it's grandparent lap node and counting how many preceding lap siblings it has. The first lap will have 0 siblings, the second lap 1, and so on.

That's it! With less than 20 lines of R the XML has been transformed into a tidy, rectangular data format, ready for visualisation and analysis. Speaking of visualisation, let's take a look at a few different aspects of the data to get a general feel for it. The following graph shows the power output over time, each lap being coloured separately. Laps one and three contain the data that will be used in the model.

```{r echo=FALSE}
cycle_data %>% 
    ggplot() +
    geom_line(aes(time, power, color = as.factor(lap)), size = .4) +
    labs(
        title = 'Track Cycling - Power over Time',
        subtitle = 'Laps differentiated by colour',
        x = 'Time',
        y = 'Power (Watts)',
        colour = 'Lap'
    )
```

The data was generated on a track bike which has only a single gear, so the speed and cadence should have a near perfect linear relationship:

```{r echo=FALSE}
cycle_data %>% 
    ggplot() +
    geom_point(aes(cadence, speed), size = .4) +
    labs(
        title = 'Speed versus Cadence',
        subtitle = 'Data generated on a single-gear track bike',
        x = 'Cadence (RPM)',
        y = 'Speed (Metres/Second)'
    )
```

There's a clear linear relationship, but there is also distribution of speeds across each cadence value. This is likely due to the difference in precision between the cadence and the speed, as cadence is measured as a integer whereas speed is a double with a single decimal point[^2].

[^2]: A linear regression of cadence on speed was performed, and the residuals were in the range of (-.5, .5). This supports our precision difference hypothesis.

Before we look at the power and speed, we need to do a little bit of housework. The second and fourth laps are extracted, and a new *position* factor variable is created with nicely named levels. 

In what could be considered controversial, we're going to remove data points where the bike was accelerating - i.e. the rate of change of the power between trackpoint samples was between -10 and 10 watts. Acceleration was required to 'move' to different speed increments, but our model only relates to points of (relatively) constant speed. Given our knowledge of the data generation process, I think this data removal can be justified.


```{r}
cycle_data_cleaned <-
    cycle_data %>% 
    filter(
        lap %in% c(1,3),
        between(power - lag(power), -10, 10)
    ) %>%
    mutate(position = fct_recode(as_factor(lap), "Tops" = "1", "Drops" = "3"))
```

We can now view the power output versus the speed by position of the data we'll be using in our model.      

```{r echo=FALSE}
cycle_data_cleaned %>% 
    ggplot() +
    geom_point(aes(speed, power, colour = position), size = .8) +
    labs(
        x = 'Speed (Metres/Second)',
        y = 'Power (Watts)',
        title = 'Speed versus Power',
        colour = 'Hand Position'
    )
```

We see some sort of exponential relationship between speed and power (we'll discuss that in the next section). We can also see the "blobs" of data where I have tried to keep a constant speed, and how keeping that constant speed become more difficult as I went faster. What is not instantly visible is the difference in power output versus speed for each of the different hand positions.


# Defining and Building a Model

Before we build our model in R we first have to define what the model is going to be. We'll be using the going to be using the classic drag equation: 
y
$$ F_d = \frac{1}{2}\rho C_D A v^2$$
This says that the force of drag \\(F_d\\) on a body when moving through a fluid is proportional to half of the density of the fluid (\\(\rho\\)) times the drag coefficient of my bike/body (\\(C_D\\)) time  is front on cross-sectional area (\\(A\\)) times the square of my is my velocity (\\(v\\)). I'm going to bundle up all coefficients into a single coefficient \\(\beta\\).

$$ \text{Let } \beta = \frac{1}{2} \rho C_D A $$
$$ F_d = \beta v^2 $$
We've got force on our left-hand side, but we need power. Energy is force times distance, and power is energy over time, so we have:

$$ F_d \Big( \frac{x}{t} \Big) = \beta v^2 \Big( \frac{x}{t} \Big)$$ 
Distance over time is velocity so we are left with:

$$ P_d = \beta v^3 $$ 
The coefficient is conditional on the position variable, so we'll end up with two coefficients from this model:

$$
  P_d  =\left\{
  \begin{array}{@{}ll@{}}
    \beta_{tops} v^3 & \text{if}\ position = tops \\
    \beta_{drops} v^3 & \text{if}\ position = drops
  \end{array}\right.
$$

Is this a perfect model? Not at all, but for our purposes it should be reasonable. Don't make me tap the "all models are wrong..." sign!

The model will give us an estimate (with some uncertainty) \\(\beta_{tops}\\) value when I was on the tops of the handlebars, and a \\(\beta_{drops}\\) value when I was in the drops.

We have some prior information that we can be included in the model: it takes zero watts to go zero metres per second. This implies that our model should go through the origin \\((0,0)\\) and we should not include an intercept. I believe that given our strong knowledge of the process that generated the data, removing the intercept is valid.

```{r}
cycle_data_mdl <-
    cycle_data_cleaned %>% 
    lm(power ~ 0 + position:I(speed^3), data = .) 
```

Here's what model looks like overlayed on the data:

```{r echo = FALSE}
crossing(
    speed = seq(0, 14, by = .1),
    position = as_factor(c('Tops', 'Drops'))
) %>%  
    mutate(
        power = predict(
            cycle_data_mdl,
            newdata = tibble(speed = speed, position = position)
        )
    ) %>%
    ggplot() +
    geom_line(aes(speed, power, colour = position)) +
    geom_point(
        aes(speed, power, colour = position),
        size = .4,
        data = cycle_data_cleaned,
    ) +
    labs(
        title = 'Speed over Power',
        subtitle = 'power ~ 0 + position x speed^3',
        x = 'Speed (Metres/Second)',
        y = 'Power (Watts)',
        colour = 'Hand Position'
    )
```

As expected the drops is more efficient that the tops. Before looking at the parameters of the model let's first look at some diagnostics. The first one to look at is the fitted values of the over the residuals:

```{r echo=FALSE}
cycle_data_mdl %>%
    augment() %>%  
    ggplot(aes(.fitted, .std.resid)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = 'y~x') +
    labs(
        title = 'Fitted over Standardised Residuals',
        x = 'Fitted Value (Watts)',
        y = 'Standard Residual',
    )
```

I've added a linear regression line to highlight the trend, and it shows shows something quite interesting: there appears to be a linear relationship that our model hasn't accounted for.

If we think back to our model, we were only accounting for the power required to overcome drag, but there's another force in play that we've completely ignored: friction. There's the rolling friction of the wheels on the tack, and the sliding friction of the hubs, the chainset and pedals, and of the chain on the sprocket.

With this realisation, let's try and build a better model to account for this force.

# Building a Better Model

In the original model, \\(P_{Total} = P_{Drag}\\), but in our updated model total power used is made up of power to overcome drag plus power to overcome friction:

$$ P_{t} = P_{d} + P_{f} $$
Once again knowing that forces times distance is energy, and energy over time is power, we end up with:

$$ P_{f} = \frac{ F_{f} \times x }{ t } = F_{f}v $$

If we let \\(\beta_1 = F_{f}\\) then our updated model is:

$$
  P_d =  \beta_1 v + \left\{
  \begin{array}{@{}ll@{}}
    \beta_{tops} v^3 & \text{if}\ position = tops \\
    \beta_{drops} v^3 & \text{if}\ position = drops
  \end{array}\right.
$$
We'll now run our updated model over the data. The frictional component is not going to be affected by the position on the handlebars, so we ensure it's not conditional on the position:

```{r}
cycle_data_mdl <-
    cycle_data_cleaned %>% 
    lm(power ~ 0 + speed + position:I(speed^3), data = .) 
```

Here's the updated on model on top of the original data:

```{r echo = FALSE}
crossing(
    speed = seq(0, 14, by = .1),
    position = as_factor(c('Tops', 'Drops'))
) %>%  
    mutate(
        power = predict(
            cycle_data_mdl,
            newdata = tibble(speed = speed, position = position)
        )
    ) %>%
    ggplot() +
    geom_line(aes(speed, power, colour = position)) +
    geom_point(
        aes(speed, power, colour = position),
        size = .4,
        data = cycle_data_cleaned,
    ) +
    labs(
        title = 'Speed over Power',
        subtitle = 'power ~ 0 + speed + position x speed^3',
        x = 'Speed (Metres/Second)',
        y = 'Power (Watts)',
        colour = 'Hand Position'
    )
```

Hard to discern if much difference from this graph, so we'll go back and look at our fitted versus residual diagnostic graph:

```{r echo=FALSE}
cycle_data_mdl %>%
    augment() %>%  
    ggplot(aes(.fitted, .std.resid)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = 'y~x') +
    labs(
        title = 'Fitted over Standardised Residuals',
        x = 'Fitted Value (Watts)',
        y = 'Standard Residual',
    )
```
That's looking much better! We've now captured the linear component, the residuals are random, and the variation is reasonably even across the entire spread of fitted values. There are a few outliers, and a more rigourous analysis would look to determine whether they had significant leverage on our regression line. Subjectively looking at this graph though my guess would be no.

The other type of diagnostic we'll look at is a histogram of the residuals. A linear regression has an assumption that the residuals are normal. The residual shape doesn't affect the point estimates of the model, but does affect the confidence intervals of the parameters.

```{r echo=FALSE}
cycle_data_mdl %>%
    augment() %>%  
    ggplot() +
    geom_histogram(aes(.std.resid), binwidth = .2) +
    labs(
        title = 'Speed versus Power Model - Residual Histogram',
        subtitle = 'power ~ 0 + speed + position x speed^3',
        x = 'Standardised Residual (Bucket Size = .2)',
        y = 'Count'
    )
```

This looks great: the residuals have an approximate Gaussian shape, there's not much mass at more that 2 standard deviations, and the mean sits approximately at zero.

With confidence in the model we now take a look at the parameters:

```{r, echo = FALSE}
beta_1 <- tidy(cycle_data_mdl)[[2]][1]
beta_tops <- tidy(cycle_data_mdl)[[2]][2]
beta_drops <- tidy(cycle_data_mdl)[[2]][3]
percent_change <- ((beta_tops- beta_drops) / beta_tops) * 100

tidy(cycle_data_mdl) %>% 
    janitor::clean_names(case = 'title') %>% 
    knitr::kable()
```

The speed term is the \\(\beta_1\\) coefficient, which is the the frictional force of the bike. The model has determined that the frictional of the bike accounts for `r round(beta_1, 2)` Newtons of force.

The next two are the coefficients of the \\(v^3\\) term when the position varibale is 'Tops' and when it is 'Drops'. Value of the coefficient isn't important to us (being a combinatio of the fluid density, drag dofficient, and my cross-sectional area), but what we want to look at is the relative difference. The result is that, for a specific velocity, we need to use `r round(percent_change, 2)`% less power. Put another way, we are `r round(percent_change, 2)`% more efficient in this position.

The following table gives you an idea on the differences in power required for speeds of 20, 40, and 60 km/h.


```{r echo=FALSE, comment=""}
crossing(
    kmph = c(20, 40, 60),
    position = as_factor(c('Tops', 'Drops')),
) %>% 
    mutate(
        speed = kmph / 3.6,
        power = round(predict(
            cycle_data_mdl,
            newdata = tibble(
                speed = speed,
                position = position
            )
        ), 2)
    ) %>% 
    pivot_wider(names_from = position, values_from = power) %>% 
    select(-speed) %>% 
    rename(Speed = kmph) %>% 
    mutate(`Power Difference` = Tops - Drops) %>% 
    knitr::kable()
```

# Percentage Uncertainty

In calculating the *average* percent decrease, we've thrown away all of our uncertainty of the parameters. If we make two assumptions, both of which I beleive to be strong:

1. The parameter estimates are normally distributed, and
2. There is no covariance between the parameters

then we can take a computational approach to determining the uncertainly of the percentage. We take one million samples from a normal distribution for each of the parameters, with a mean of the estimate, and a standard deviation  of the standard error. We can then calculate the percentage for the one million pairs of parameters generated[^3] which gives us a distribution of percentages, the look at the quartiles for these values.

[^3]: Thanks to /u/eatthepieguy for responding to my [query on this](https://www.reddit.com/r/statistics/comments/sehzun/q_confidence_intervals_for_percentages/).

```{r}
beta_tops <- tidy(cycle_data_mdl)[[2]][2]
sigma_tops <- tidy(cycle_data_mdl)[[3]][2]
beta_drops <- tidy(cycle_data_mdl)[[2]][3]
sigma_drops <- tidy(cycle_data_mdl)[[3]][3]

percent_distribution <-
    tibble(
        beta_top_dist = rnorm(1000000, beta_tops, sigma_tops),
        beta_drop_dist = rnorm(1000000, beta_drops, sigma_drops),
        percent = ((beta_top_dist - beta_drop_dist) / beta_top_dist) * 100
    )

percent_distribution %>% 
    ggplot(aes(percent)) +
    geom_histogram( binwidth = .1) +
    labs(
        title = 'Distribution of Computed Parameter Percentage Increases/Decreases',
        x = "Percentage Increase/Decrease",
        y = 'Count (Bin Width = .1)',
    )
```
Our 89%[^4] confidence interval is therefore:
```{r, echo = FALSE}
percent_distribution %>% 
    summarise(quantile = c(.055, .945), range = quantile(percent, probs = quantile)) %>% 
    mutate(quantile = percent(quantile, accuracy = 0.1)) %>%
    janitor::clean_names(case = 'title') %>% 
    knitr::kable()
```
[^4]: Why 89%? Well, why 95%?

# Summary

In this article we looked at the aerodynamics of different positions on a bike. We gathered data using different sensors, and showed the elegance of R by transforming XML data into a rectangular, tidy data frame.

We defined a simple model and used this to perform a regression of power required to maintain a specific velocity. By performaing diagnostics on this model, we were able to identify that our model was incomplete, and that we were likely not including friction in the model. We defined and created a new model with friction included, which performed better than our original model.

The ultimate aim of the article was to determine how much more efficient it is to ride in the 'drops' of the handlebars rather than the 'tops'. From our modelling we found the average estimate of our efficiency gain to be `r round(percent_change, 2)`%, with an 89% confidence interval of [6.7%, 15.7%].


