---
title: 'A Tale Of Two Optimisations'
author: 'Greg Foletta'
date: '2021-09-20'
slug: 'tale-of-two-optimisations'
categories: [R, C]
---

A couple of month's ago I wrote a toy program called 'whitespacer'. It takes bytes and encodes/decodes them from/to whitespace.

Ever since I wrote it, it had been gnawing at me as to whether I could have written it in a more performant manner. I had an idea about trying to define a mathematical function that could perform the encoding, rather than relying on a lookup table.

This article has somewhat of a multiple personality. In the first half I'll work through attempting to optimise the program and benchmarking it. But the benchmarking revealed something interesting about branch prediction that I felt needed to be investigated as well, so in the second half we'll take a diversion into that. 

I'm hoping that the two sections aren't too disjointed.




```{r, include=FALSE}
library(tidyverse)
library(gt)
library(glue)
library(cowplot)
```


# Attempt 1: A Mathematical Function

In the first iteration of the whitespacer program I used lookup tables to encode/decode the bytes. One lookup table took a dibit (two bits) of value 0 - 3 and mapped this to a whitespace character. Another lookup table did the inverse. Here's the lookup table encoder function, which I've factored out:

```C
//Dibit to whitesapce lookup table
char encode_lookup_tbl[] = { '\t', '\n', '\r', ' ' };

//Given a dibit, returns the whitespace encoding
unsigned char lookup_encode(const unsigned char dibit) {
    return encode_lookup_tbl[ dibit ];
}
```

I thought if I could find a mathematical function to perform this mapping, rather than a lookup table, I might be able to save some time on memory accesses in the hot encoding and decoding loop.

From somewhere in the recesses of my brain I recalled Lagrange polynomials: if we have a set of $k + 1$ data points $(x_0, y_0), \ldots, (x_k, y_k)$ where no two $x_i$ are the same, a polynomial of degree $k$ or less can be defined that passes through all the points. For us that means I can define a function:

$$ f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 $$

which takes a dibit of 0, 1, 2 or 3 and returns the appropriate whitespace character. I can also define the inverse function (with differing $\beta$ coefficients) which does the inverse. Outside of the $[0, 3]$ range the function won't make much sense, but that fine for our use case.


```{r echo=FALSE}
whitespace <-
    tibble(
        dibit = 0:3,
        whitespace = c('\t', '\n', '\r', ' '),
    ) %>% 
    mutate(ascii_dec = map_int(whitespace, ~utf8ToInt(.x)))

whitespace %>%
    select(-whitespace) %>% 
    gt() %>% 
    cols_label(
        dibit = 'Dibit',
        ascii_dec = 'ASCII Decimal Value'
    )
```
I now create two models: one for encoding, and one for decoding. I'm using a linear regression rather than the Lagrange polynomial formula, but the $\beta$ coefficients end up the same.

```{r}
encode_model <-
    whitespace %>% 
    lm(ascii_dec ~ dibit + I(dibit^2) + I(dibit^3), data = .)

decode_model <-
    whitespace %>% 
    lm(dibit ~ ascii_dec + I(ascii_dec^2) + I(ascii_dec^3), data = .)
```

Visualising these models will help us see what's going on. On the left is out encoding function, which takes out dibit values and maps them to our ASCII whitespace characters. On the right is the inverse function, which takes the ASCII whitespace characters' decimal value and maps it back to a dibit. This function shows how the function makes little sense outside of our four defined values.

```{r echo=FALSE}
plot_grid(
    whitespace %>%
        ggplot() +
        geom_point(aes(dibit, ascii_dec)) +
        geom_function(fun = ~{ predict(encode_model, tibble(dibit = .x))}) +
        labs(
            x = 'Dibit',
            y = 'ASCII Decimal',
            title = 'Whitespace Encoding Function'
        ),
    whitespace %>%
        ggplot() +
        geom_point(aes(ascii_dec, dibit)) +
        geom_function(fun = ~{ predict(decode_model, tibble(ascii_dec = .x))}) +
        labs(
            x = 'ASCII Decimal',
            y = 'Dibit',
            title = 'Whitespace Decoding Function'
        )
)
```

Let's take a look at the $\beta$ coefficients.

```{r}
tibble(
    parameter = c('beta_0', 'beta_1', 'beta_2', 'beta_3'),
    encode = encode_model %>% coef(),
    decode = decode_model %>% coef()
) %>% 
    gt() %>% 
    cols_label(
        parameter = 'Parameter',
        encode = 'Encoding',
        decode = 'Decoding'
    )
```
We've got a  bit of a problem. I'll be honest, I was hoping - somewhat optimistically - that I would get some nice, clean integer coefficients to work work.

Let's persist and create encoding and decoding functions based on polynomial functions. 

```C
unsigned char poly_encode(const unsigned char dibit) {
    return 9.0 + 
    4.666667 * dibit - 
    6.0 * (dibit * dibit) + 
    2.333333333333 * (dibit * dibit * dibit);
}
```

The cast from floating point to unsigned char gi

# Attempt 2: A Switch

While working on the polynomial functions, I the thought that, rather than using a lookup table or a mathematical function, I simply use a switch statement. It looks at the value of the dibit and simply returns the whitespace character. Here's the implementation of the encoding function:

```C
unsigned char switch_encode(const unsigned char dibit) {
    switch (dibit) {
        case 0:
            return '\t';
        case 1:
            return '\n';
        case 2:
            return '\r';
        case 3:
            return ' ';
    }
}
```

Finally, we need a way to select the algorithm the program uses at runtime. I added the '-a <algorithm>' command line argument to allow this selection, where <algorithm> is either 'lookup', 'poly' or 'switch' to select the algorithm to use. If none is specified it defaults to the original lookup table.


# Profiling

```{zsh include=FALSE}
git clone --branch algorithms git@github.com:gregfoletta/whitespacer.git
cd whitespacer
make debug
cp ws_debug ..
cd ..
dd if=/dev/urandom of=urandom_32M bs=1MB count=32
```

Rather than supposition, let's test our assumptions and see where they land. I've created a small R function which takes a vector of shell commands and returns how long they took to run. There's a small amount of overhead in spawning a shell, but this is constant across and as we're looking at he *differences* between the runtimes, it gets cancelled out.

```{r system_profile}
system_profile <- function(commands) {
    map_dbl(commands, ~{
        start <- proc.time()["elapsed"] 
        system(.x, ignore.stdout = TRUE)
        finish <- proc.time()["elapsed"]
        finish - start
    })
}
```

I run 500 iterations of an encode / decode pipeline for each algorithm. I pipe in a 32Mb file of random bytes generated from */dev/urandom*, and piping the output to */dev/null*. We end up with a table of 1500 rows with the time it took to run this pipeline.

```{r, cache = TRUE}
profiling_results <-
    tibble(
        n = 1:900,
        algo = rep(c('lookup', 'poly', 'switch'), max(n) / 3)
    ) %>% 
    mutate(
            command = glue('cat urandom_32M | ./ws_debug -a { algo } | ./ws_debug -d -a { algo } > /dev/null'),
            time = system_profile(command)
    ) %>% 
    select(-command)
```

Rather than simply looking at the means or medians for each of the algorithms, we take a look at the distribution of runtimes for each with the mean highlighted.

```{r echo=FALSE}
profiling_means <-
    profiling_results %>% 
    group_by(algo) %>% 
    summarise(mean = mean(time))

profiling_results %>% 
    ggplot() +
    geom_density(aes(time, fill = algo), alpha = .4) +
    geom_vline(data = profiling_means, aes(xintercept = mean, colour = algo)) +
    geom_label(data = profiling_means, aes(x = mean, y = 0, label = round(mean, 2))) +
    labs(
        x = 'Seconds',
        y = 'Density',
        title = 'Whitespacer Algorithms - Performance',
        fill = 'Algorithm',
        colour = 'Algorithm'
    )
```

As I kind-of expected, the polynomial encoding/decoding is slower than the lookup table. But what is really surprising is that the switch statement is even slower. On average it's taking over one second more to encode and decode the 32M file. I love a good surprise, so let's dive in and find out what happening.

# What's With The Switch?

I took a bottom-up approach to trying to solve this mystery. This made sense because:

a. I was the one that wrote it.
b. The cod is simple and
c. This is a learning experience.

In most other scenarios I think a top-down approach would be the more efficient.

The approach taken was to first take a look at the instructions that were being executed on the CPU as the program was running. We can use `perf record` execute the program take samples of it's state, which on my machine will use the *Precise Event Based Sampling (PEBS)* Intel feature. At a certain frequency (default 4000Hz, based on an overflowing counter) the PEBS will write processor state (CPU ID, instruction pointer, register values, etc) to a buffer, issue an interrupt, and perf will read these values.

By using the `-b` switch, perf also captures the *Last Branch Record (LBR) stack*. With the LBR processor feature, the CPU logs a set of *from* and *to* address of branches taken (predicted and mispredicted) to a set of special purpose registers in a ring buffer (my CPU has 32 entries in the ruing buffer). With this information perf can reconstitute the history of instructions executed on the CPU, rather than only having a single point - the instruction pointer - to use from the sample.

Perf runs the whitespacer encoding half of the pipeline, and is fed 32Mb of random bytes.

```sh
perf record -b -o switch.data -e cycles:pp ./ws_debug -a switch < urandom_32M > /dev/null
[ perf record: Woken up 22 times to write data ]
[ perf record: Captured and wrote 5.345 MB switch.data (6833 samples) ]
```

Looking at the trace data (saved in *switch.data*), the *brstackins* field allows us to see the assembly instructions executed along the branches of the branch stack. Perf has captures around 43,000 executions of our `switch_encode()` function. Here's the output from one of them:

```sh
perf script -F +brstackinsn -i switch.data
```

```asm
switch_encode:
0000560dd2bffddd        insn: 55 
0000560dd2bffdde        insn: 48 89 e5 
0000560dd2bffde1        insn: 89 f8 
0000560dd2bffde3        insn: 88 45 fc 
0000560dd2bffde6        insn: 0f b6 45 fc 
0000560dd2bffdea        insn: 83 f8 01 
0000560dd2bffded        insn: 74 1e                     # MISPRED 4 cycles 1.50 IPC
0000560dd2bffe0d        insn: b8 0a 00 00 00 
0000560dd2bffe12        insn: eb 0e                     # PRED 22 cycles 0.05 IPC
0000560dd2bffe22        insn: 5d 
0000560dd2bffe23        insn: c3                        # PRED 5 cycles 0.20 IPC
```     

The thing that stands out immediately is the misprediction, and the wopping 22 cycles in the other predicted branch. Let's take a step back and run the `perf stat` command to pull out to pull out some summaries:

```{sh}
# Perf stat with 32Mb urandom bytes
perf stat ./ws_debug -a switch < urandom_32M > /dev/null       
```

The first statistic that should conern us is the instructions per cycle, which is just under one. The second is the numbner of branch misses, which is up at 12.56%. Put simply, we're not getting much bang for our buck when it comes to predicting branches.

The hypothesis at this point is that given our data is random and uniformly distributed, combined with the branching (the C switch statement) used to encode the data, the branch predictor is performing poorly, and so is our performance. 

To confirm this hypothesis, let's create an input file of all zeros and see how it performs. 

```{zsh zero_file, include=FALSE}
# Create a 32M file of all zero bytes
dd if=/dev/zero of=zero_32M bs=1MB count=32
```

```{zsh perf_stat_switch_zero}
```

There we go - the number of instructions has remained almost the same, but we're getting 3.2 instructions per cycle. This appears to be due to the number of branch misses approaching 0.

I'll be honest and say that at this point I'm butting up against the limits of my CPU architecture knowledge. I thought that branch predictions were a zero-cost optimisation, and that mispredicted branches didn't have any side effects. If anyone has any more information or theories on this please [reach out](mailto:greg@foletta.org).

# Lookup versus Polynomial

Now let's take a look at the unsurprising result: our original lookup algorithm versus the polynomial. We'll go straight to using `perf stat` to see high-level statistics:

```{zsh}
# Lookup table 
perf stat ./ws_debug -a lookup < urandom_32M > /dev/null
```

```{zsh}
# Polynomial
perf stat ./ws_debug -a poly < urandom_32M > /dev/null
```

We see two main reasons as to why the polynomial is slower. First off it simply takes for instructions to calculate the polynomial, as compared to simply looking up the value in a lookup table. Secondly, this increase in instructions is compounded by that fact that we're getting less instructions per seconds.

# Summary

In this article we looked at two different alternatives to a lookup table for encoding and decoding bytes in the *whitespacer* program. The fist was to try and use a polynomial function, and the seconds was to use a switch statement rather than a lookup table.

Upon calculation of the coefficients for the polynomial, we knew we


```{zsh}
rm -rf whitespacer urandom_32M zero_32M ws_debug 
```

